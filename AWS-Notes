AWS Solutions Architect Certification Notes:

-IAM(Identity Aceess Management):
Allows you to manage users and their level of access to the AWS console.

1. Centralised control of aws acc
2. shared access 
3. Granular Permissions
4.Identity Federation(Allows users to access the AWS using theirLinkedIn or FB creds)
5. Multifactor Authentication
6. Provides Temporary Access
7. Own Password Rotation Policy
8. Supports PCI DSS Compliance(All Personal Data is secured)


Key Terms in IAM:
1. Users: End users such as people, employees etc
2. Groups: A collection of Users. Each user in the group will have specific permissions assigned to that group
3. Policies: Made up of docs. Formatted in JSON.Gives permissions on what the user/group is able to do
4. Roles: Create roles for each user which provides access for each stack in AWS. For ex we can request role access to a EC2 console to write files in the S3 bucket.

Root Account: The credentials you used to login in the first time. This creds will have god mode access
Whatever changes we do in the IAM it will be implemented on the global basis. 
NEW USERS WILL HAVE NO PERMISSIONS WHEN FIRST CREATED.
NEW USERS are assigned Access Key ID and Secret Access Keys when first created. 
The Access Key ID and Secret Access Keys are not Id and Password. We can access AWS via the API and command line.
ALWAYS SETUP MULTIFACTOR AUTHENTICATION for your root account.(Google Authenticator App)

Hands On:
1. First always to Enable Multi Factor Authentication. Install Google Authenticator in the phone, scan the barcode and enter the MFA code 2 times(after first one expires). 
2. Need to add user in the IAM role. This can be customized to provide programatic access or use a custom password.
Once the custom password is created, We can download the credentials.csv file which would have all the credentials needed for your IAM Role.
3. Next we need to provide permissions. We can start by creating a Group. Upon providing a name to the group we can see we can add policies to the group.
Few policies are AWS managed Policies which covers almost all the access policies based on the role.
4. Once the policies are added to the group. The user is already added to the group. So the user will have all the access mentioned in the policies for that group.
5. Lastly we need to setup IAM Password Policy. This includes the length of password, Frequency to change the password etc.

Major point to remmember is never to share the access key id and secret access key with anyone. This would give access to the complete AWS console using these credentials.

Finally we need to assign roles. This givess access to a AWS service to access another AWS service. We can go to role, Add role, Select the service for which you want access.
For ex EC2, and provide a policy for this role(Here We can give S3FullAccess(AWS Managed policy) to give the EC2 complete access to the S3 bucket)




S3 (Simple Storage Service)
-S3 provides developers with Secure, durable and highly scalable object storage.
-Simple webservice interface to store and retrive any amount of data from anywhere on the web.
-safe place to store your files. It is object based storage.
-Files can range from 0-5tb
-Files are stored in buckets.Kind of folders.
-S3 follows a universal Namespace. The names must be unique globally. This is because it creates a webaddress on its name.
-Once we upload a file on S3. We will receive a HTTP200 code in our browser saying the upload is successful.


S3 Object consists of the following:
1. Key: Its the name of the object or the file
2. Value: This is the data and is made up of sequence of bytes
3. Version ID: It is used for versioning to have version control.
4. Metadata: Its the data about the data we are storing(Finance, payroll data etc.)
5. Access control and torrents

How does data consistency work for S3?
-S3 has Read after Write consistency for PUTS of new objects. This means if a new file is placed in S3. We can immediately read the file once its posted.
-S3 has eventual consistency for all overwrites, updates and deletes of the existing S3 PUTS objects.
(This means once we do an update to an existing S3 object, It will immediately show both the objects but eventually will get the most updated one)


S3 Features:
1. Tiered Storage Options
2. Lifecycle Management(Options to push data after certain timeline to another storage)
3. Versioning
4. Encryption
5. MFA Delete (Use Multifactor Authentication to delete)
6. Secure data using Access Control Lists and Bucket policies.

Types of S3 Storage Classes:(Need to remmember this for your exam)
1.S3 STANDARD:
-99.99% Availibility
-99.99999999999% Durability
-Designed to sustain loss of 2 facilities concurrently.

2.S3-IA(Infrequently Accessed):
-For data that is accessed less frequently
-Provides Rapid access when needed.
-Lower fee than S3 but you are charged a retrival fee.

3. S3 One Zone-IA:
-Low cost option for IA data.
- Stored in one Availibility zone.

4. S3- Intelligent Tiering
-Uses ML to optimize costs by automatically moving data to the most cost effective access tier without performance impact or operational overhead.

5. S3 Glacier:
-Used for secure, durable and low cost storage class for data archiving.
-costs are cheaper than on premises solutions.
-Retrieval times are configurable from minutes to hours.

6. S3 Glacier Deep Archive:
-S3 lowest cost storage class
-Retreival Time of 12 hours is acceptable.



S3 charges:
-Based on Storage
-Requests
-Storage Management Pricing(Different Tiers)
-Data Transfer Pricing
-Transfer Acceleration: enables fast,easy and secure transfer of files over long distances. 
-Cross Region Replication:




Creating S3 Bucket Steps:
1. Create S3 will be in global region.
2. The name is also unique globally
3. While creating the bucket the default settings is to block all public access. If we place a file in a bucket. A URL is created to access
 the file. But upon clicking, The file does not open and pops an xml webpage with access denied.
4. To make a file public. We first need to make the bucket to give public access. By changing this settings, The existing files within 
the bucked do not become public. To make them public, We need to select the file inside the s3 and in actions select make public. Thats when
we can see the file.
5. After uploading a file, We get a HTTP200 code
6. Control Policies to your bucket using bucket (Access Control Lists)ACL or bucket policies.

Bucket Policies: Its the config for each bucket.
ACL: Its for individual objects
S3 can be configures to create access logs which log all requests made to s3 bucket. This can be sent to another bucket to the same acc or
different acc.

ENCRPYTION is used when we transfer files to s3.(https). It is achieved by SSL/TLS.

Encryption can be usually done on the service side(where aws encrypts the file in bucket) or 
client side(where we encrypt the file and upload to s3).

Different ways with which we can encrypt on service side are:
1. S3 Managed Keys (Service Side Encryption)SSE-S3 (AWS manages the keys)
2. AWS Key Management Service, Managed Keys - SSE-KMS (AWS and client manages keys)
3. Server Side Encryption with Customer Keys - SSE-C (Customer encrypts the file in AWS)


S3 Versioning:
-Stores all versions of an object(Even edits and deletes)
-Handy Backup Tool
-Once enabled, Versioning cannot be disabled. It can only be suspended.
-It integrates with lifecycle rules
-Versioning can be enabled with MFA delete.


For an existing file in S3. If we download and do changes. Then upload the same file into s3. The permissions for the uploaded file will be
blocked to public access by default. Even if the s3 bucket has enabled public access.
*** If versioning is enabled in a S3. It means the space utilization is going to increase for each version. This is important from an 
architectural point of view.***

-After hiding version. If we delete an object in s3. We can see that after enabling version in view, The most recent file just has a 
delete tag on it. To delete the file. 
-we need to go to actions and then delete. 
-This way the delete request gets deleted and the version of the file which we wanted to delete will be the most recent version of that file.
-Think of it like delete does not delete the file but creates a new version of that file with a mark as delete tag.
-If we again delete the latest version of the file. Then it goes back to the previous version of the same file.That becomes the latest
version.

Life Cycle Management: It helps automating moving objects to different storage tiers.
-We can create Life Cycle management rules for our s3 bucket. 
- WE can mention the rules specific for current version and previous versions.
-For ex: we can say to push all objects in current bucket to another S3-IA bucket after 30 days. and move the same object to Glacier after
60 days

Cross Region Replication:
-To enable this, We need to enable versioning.
-The replication needs to be in another region.
-WE can change the storage class for the replicated s3
-We can change the ownership to the destination s3 bucket account
-Once these are selected, WE need to provide a IAM role for this process. We can create one if needed.

Once cross region replication is enabled, It does not immediately copy all the files to the new bucket.
But, when we start making changes to the objects in the 1st bucket, The current version of the object is uploaded to the CRR buckt as well.
-If we try to delete the current version of the object in the first bucket, This will not delete in the CRR bucket. 

Transfer Accleration:
-This is an additional option to transfer files to s3. Transfering Files using Transfer Accleration will let the customer upload the file
to the edge location and the edge location uploads the file to the s3. There is a website with amazon which helps you to test the transfer accleration
speeds with normal s3 to s3 and using transfer accleration methods. 

AWS CloudFront:
It is a Content Delivery Network(CDN), which is a system of distributed servers that deliver webpages and other web content to users based on 
geographical location of the user.

Terms used:
-Edge Location:We know it
-Origin: The origin of files where the CDN will distribute. This can be an s3 bucket, EC2 instance elastic load balancer or route 53.
-Distribution: Name given to CDN which consists of all edge locations.

When a customer requests a file from origin. The distribution downloads the file from origin to edge location. This can be configured to
store the file in the edge location.
There are 2 types of CloudFront:
1. Web Distribution- Used for websites.
2. RTMP: Used for adobe and media streaming.

**Edge locations are not just READ only, We can write on them too
**Objects are cached for the life of TTL(Time to Live).
**We can invalidate cached objects. We will be charged for this service


***AWS SNOWBALL***
Service which can import to s3 and export from s3.
- SNOWBALL is a petabyte-scale data transport solution that uses secure appliances to transfer large amounts of data in and out of AWS.
- Simple, Fast, Secure and can cost as less as 1/5th of the cost of high speed internet.
- It comes in 50TB or 80TB. 
- Uses Multiple layers of security including tampler-resistant enclosures, 256-bit encryption and an industry standart Trusted Platform Module(TPM)
- Once data transfer is done and verified, AWS performs a software erasure of the snowball appliance.

- AWS SNOWBALL EDGE comes with 100tb of data. It comes with Storage and compute capabilities. 
- For ex: we can create Snowball Edge to not just store data but also to perform few operations or lambda functions in it.

-AWS SNOWMOBILE is an exabyte-scale data transfer service. Can transfer upto 100PB per snowmobile.


------AWS STORAGE GATEWAY------
Its a service that lets us connect existing on premise software applications with cloud based storage to provide secure integration
between on premise IT with AWS storage.
Three types of Storage Gateways:
1. File Gateway: Its used to store files in s3. can be configured with NFS(Network File System) and SMB(Server Message Block)
2. Volume Gateway: Its used to store the copies of HD or Virtual HD in s3. This can be configured for STORED VOLUMES or CACHED Volumes.
3. Tape Gateway: This is a virtual Tape Library(VTL)


1.File Gateway:
-The files are stored in s3 and are accessed through NFS
-Ownership, Permissions and timestamps are durably stored in s3 in the user-metadata of the object associated with the file.
-Once the objects are transfered to s3, They can be managed as native s3 objects.
- bucket policies apply directly to the objects stored in the bucket.
APPLICATION SERVER -> STORAGE GATEWAY -> DIRECT CONNECT/INTERNET/AMAZON VPC -> S3

2. Volume Gateway:
-Presents your application with disk volumes using the iSCSI block protocol.
-Snapshots can be created to backup and it captures onlt the changed blocks.
-All snapshot storage is compressed to minimize your storage charges.
  a.Stored Volumes: They store the entire dataset in the client location. Using a iSCSI connectivity, We push all the snapshots to the 
  aws s3.
  b. Cached Volumes: It stores only the actively/recently used data.It stores data that you write to these volumns in S3 and retains recently 
  read data in your on-premisis storage gateway's cache and upload buffer storage.
  
3. Tape Gateway: We can store the existing virtual tape cartridges that you create on your tape gateway.



-----Athena vs Macie------

Athena: Interactive Query Service which enables you to analyse and query data located in s3 using standard SQL.
-serverless, pay per query, per TB scanned.
- No need to setup complex ETL
- Works directly with data stored in s3.

Where it can be used:
- commonly used to query log files
- Generate business reports on data in s3.
- Analyse AWS cost and usage reports.
- Run queries on click stream data.

Macie: Security service that uses ML and MLP to discover, classify PII in s3.
- can be used to analyse cloudtrail logs for suspicious api activity
- includes dashboards, reports and alerting.
- Great for PCI-DSS(Payment Card Industry Data Security Standard) and preventing ID Theft.

